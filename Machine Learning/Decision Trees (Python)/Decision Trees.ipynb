{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np;\n",
    "import re;\n",
    "import math;\n",
    "import matplotlib.pyplot as plt;\n",
    "import matplotlib.patches as mpatches;\n",
    "from sklearn.tree import DecisionTreeClassifier;\n",
    "from sklearn import tree;\n",
    "\n",
    "feature_file = open(\"features.txt\");\n",
    "train_file = open(\"adult_train.txt\");\n",
    "test_file = open(\"adult_test.txt\");\n",
    "\n",
    "feature_num = 12;\n",
    "train_num = 32561;\n",
    "test_num = 16281;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# parse feature maps\n",
    "feat_dict = {};\n",
    "index_map = {};\n",
    "\n",
    "new_index_map={};\n",
    "rev_index_map={};\n",
    "\n",
    "num = 0;\n",
    "for i in range(feature_num):\n",
    "    curr_line = re.split(\": |, \",re.sub(\".\\n|\\.\",\"\",feature_file.readline()));\n",
    "    \n",
    "    index_map[i] = curr_line[0];\n",
    "    \n",
    "    curr_dict = {};\n",
    "    if(curr_line[1]!=\"continuous\"):\n",
    "        curr_dict[\"continuous\"] = False;\n",
    "        for j in range(1,len(curr_line)):\n",
    "            new_index_map[num] = curr_line[0] + \"-\" + curr_line[j];\n",
    "            rev_index_map[curr_line[0] + \"-\" + curr_line[j]] = num;\n",
    "            num += 1;\n",
    "            \n",
    "            curr_dict[curr_line[j]] = 0;\n",
    "    else:\n",
    "        new_index_map[num] = curr_line[0];\n",
    "        rev_index_map[curr_line[0]] = num;\n",
    "        num += 1;\n",
    "        \n",
    "        curr_dict[\"continuous\"] = True;\n",
    "        curr_dict[\"total_val\"] = 0;\n",
    "        curr_dict[\"total_num\"] = 0;\n",
    "    feat_dict[curr_line[0]] = curr_dict;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# parse train data\n",
    "feature_set_x = np.array([]).reshape(0,len(new_index_map));\n",
    "feature_set_y = np.array([]);\n",
    "empty_features = np.array([]).reshape(0,2);\n",
    "for i in range(train_num):\n",
    "    curr_line = re.split(\", |,\",re.sub(\"\\n\",\"\",train_file.readline()));\n",
    "    curr_feat = np.zeros(len(new_index_map));\n",
    "    for j in range (len(curr_line)-1):\n",
    "        if(curr_line[j]==\"?\"):\n",
    "            empty_features = np.vstack((empty_features,[i,index_map[j]]));\n",
    "        else:\n",
    "            if(not feat_dict[index_map[j]][\"continuous\"]):\n",
    "                curr_feat[rev_index_map[index_map[j] + \"-\" + curr_line[j]]] = 1;\n",
    "                feat_dict[index_map[j]][curr_line[j]] += 1;\n",
    "            else:\n",
    "                curr_feat[rev_index_map[index_map[j]]] = int(curr_line[j]);\n",
    "                feat_dict[index_map[j]][\"total_val\"] += int(curr_line[j]);\n",
    "                feat_dict[index_map[j]][\"total_num\"] += 1;\n",
    "    y = 0 if curr_line[12]==\"<=50K\" else 1;\n",
    "    feature_set_x = np.vstack((feature_set_x,curr_feat));\n",
    "    feature_set_y = np.append(feature_set_y,y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# find mean and mode\n",
    "feat_avg_dict = {};\n",
    "for i in range(feature_num):\n",
    "    if(not feat_dict[index_map[i]][\"continuous\"]):\n",
    "        mode = max(feat_dict[index_map[i]],key=feat_dict[index_map[i]].get);\n",
    "        feat_avg_dict[index_map[i]] = mode;\n",
    "    else:\n",
    "        mean = feat_dict[index_map[i]][\"total_val\"]/feat_dict[index_map[i]][\"total_num\"];\n",
    "        feat_avg_dict[index_map[i]] = mean;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# backfill train data\n",
    "for i in range(len(empty_features)):\n",
    "    index = empty_features[i];\n",
    "    if(feat_dict[index[1]][\"continuous\"]):\n",
    "        feature_set_x[int(index[0])][rev_index_map[index[1]]] = feat_avg_dict[index[1]];\n",
    "    else:\n",
    "        mode_index = feat_avg_dict[index[1]];\n",
    "        feature_set_x[int(index[0])][rev_index_map[index[1] + \"-\" + mode_index]] = 1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# parse test data & backfill\n",
    "test_x = np.array([]).reshape(0,len(new_index_map));\n",
    "test_y = np.array([]);\n",
    "for i in range(test_num):\n",
    "    curr_line = re.split(\", |,\",re.sub(\"\\n |.\\n\",\"\",test_file.readline()));\n",
    "    curr_feat = np.zeros(len(new_index_map));\n",
    "    for j in range(len(curr_line)-1):\n",
    "        if(curr_line[j]==\"?\"):\n",
    "            if(not feat_dict[index_map[j]][\"continuous\"]):\n",
    "                mode_index = feat_avg_dict[index_map[j]];\n",
    "                curr_feat[rev_index_map[index_map[j] + \"-\" + mode_index]] = 1;\n",
    "            else:\n",
    "                curr_feat[rev_index_map[index_map[j]]] = feat_avg_dict[index_map[j]];\n",
    "        else:\n",
    "            if(not feat_dict[index_map[j]][\"continuous\"]):\n",
    "                curr_feat[rev_index_map[index_map[j] + \"-\" + curr_line[j]]] = 1;\n",
    "            else:\n",
    "                curr_feat[rev_index_map[index_map[j]]] = int(curr_line[j]);\n",
    "    y = 0 if curr_line[12]==\"<=50K\" else 1;\n",
    "    test_x = np.vstack((test_x,curr_feat));\n",
    "    test_y = np.append(test_y,y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split train/validate data\n",
    "r_state = np.random.get_state();\n",
    "np.random.shuffle(feature_set_x);\n",
    "np.random.set_state(r_state);\n",
    "np.random.shuffle(feature_set_y)\n",
    "\n",
    "split = math.floor(train_num*0.3);\n",
    "\n",
    "train_x = feature_set_x[split:];\n",
    "train_y = feature_set_y[split:];\n",
    "\n",
    "validate_x = feature_set_x[:split];\n",
    "validate_y = feature_set_y[:split];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# max_depth\n",
    "d_xaxis = np.array([]);\n",
    "d_train_acc = np.array([]);\n",
    "d_validate_acc = np.array([]);\n",
    "for i in range(1,31):\n",
    "    d_xaxis = np.append(d_xaxis, i);\n",
    "    \n",
    "    clf = DecisionTreeClassifier(max_depth=i);\n",
    "    clf.fit(train_x,y=train_y);\n",
    "    \n",
    "    y = clf.predict(train_x);\n",
    "    train_acc = np.sum(np.equal(y,train_y))/len(train_y);\n",
    "    d_train_acc = np.append(d_train_acc,train_acc);\n",
    "    \n",
    "    y = clf.predict(validate_x);\n",
    "    validate_acc = np.sum(np.equal(y,validate_y))/len(validate_y)\n",
    "    d_validate_acc= np.append(d_validate_acc,validate_acc);\n",
    "\n",
    "green_patch = mpatches.Patch(color='green', label='Validation Accuracy');\n",
    "blue_patch = mpatches.Patch(color='blue', label='Training Accuracy')\n",
    "\n",
    "plt.figure();\n",
    "plt.title(\"Max Tree Depth vs. Classificaiton Accuracy\");\n",
    "plt.xlabel(\"Max Depth\");\n",
    "plt.ylabel(\"Classification Accuracy\");\n",
    "plt.legend(handles=[green_patch, blue_patch])\n",
    "plt.plot(d_xaxis, d_train_acc);\n",
    "plt.plot(d_xaxis, d_validate_acc);\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# min samples leaf\n",
    "l_xaxis = np.array([]);\n",
    "l_train_acc = np.array([]);\n",
    "l_validate_acc = np.array([]);\n",
    "for i in range(1,51):\n",
    "    l_xaxis = np.append(l_xaxis, i);\n",
    "    \n",
    "    clf = DecisionTreeClassifier(min_samples_leaf=i);\n",
    "    clf.fit(train_x,y=train_y);\n",
    "    \n",
    "    y = clf.predict(train_x);\n",
    "    train_acc = np.sum(np.equal(y,train_y))/len(train_y);\n",
    "    l_train_acc = np.append(l_train_acc,train_acc);\n",
    "    \n",
    "    y = clf.predict(validate_x);\n",
    "    validate_acc = np.sum(np.equal(y,validate_y))/len(validate_y)\n",
    "    l_validate_acc = np.append(l_validate_acc,validate_acc);\n",
    "\n",
    "green_patch = mpatches.Patch(color='green', label='Validation Accuracy');\n",
    "blue_patch = mpatches.Patch(color='blue', label='Training Accuracy')\n",
    "\n",
    "plt.figure();\n",
    "plt.title(\"Min Samples Required to be a Leaf vs. Clasification Accuracy\");\n",
    "plt.xlabel(\"Min Samples\");\n",
    "plt.ylabel(\"Classification Accuracy\");\n",
    "plt.legend(handles=[green_patch, blue_patch])\n",
    "plt.plot(l_xaxis, l_train_acc);\n",
    "plt.plot(l_xaxis, l_validate_acc);\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.854013104013\n"
     ]
    }
   ],
   "source": [
    "# max depth = 12 -- 0.858005733006\n",
    "# min leaf  = 21 -- 0.855855855856\n",
    "# optimal classifer = 0.857493857494\n",
    "\n",
    "# optimal classifier\n",
    "clf = DecisionTreeClassifier(max_depth = 12, min_samples_leaf=21);\n",
    "clf.fit(train_x,y=train_y);\n",
    "y = clf.predict(validate_x);\n",
    "acc = np.sum(np.equal(y,validate_y))/len(validate_y);\n",
    "print(acc);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tree graph\n",
    "tree.export_graphviz(clf, out_file=\"tree.dot\", feature_names = list(rev_index_map.keys()), max_depth=3,\n",
    "                     class_names=list([\"<=50\",\">51\"]), filled=True, rounded=True,  special_characters=False);  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.860205147104\n"
     ]
    }
   ],
   "source": [
    "# test classifer\n",
    "clf.fit(feature_set_x, y=feature_set_y);\n",
    "y = clf.predict(test_x);\n",
    "acc = np.sum(np.equal(y,test_y))/len(test_y);\n",
    "print(acc);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
