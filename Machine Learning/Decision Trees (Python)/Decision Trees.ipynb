{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np;\n",
    "import re;\n",
    "import math;\n",
    "import matplotlib.pyplot as plt;\n",
    "import matplotlib.patches as mpatches;\n",
    "from sklearn.tree import DecisionTreeClassifier;\n",
    "\n",
    "feature_file = open(\"features.txt\");\n",
    "train_file = open(\"adult_train.txt\");\n",
    "test_file = open(\"adult_test.txt\");\n",
    "\n",
    "feature_num = 12;\n",
    "train_num = 32561;\n",
    "test_num = 16281;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# parse feature maps\n",
    "feat_dict = {};\n",
    "index_map = {};\n",
    "\n",
    "new_index_map={};\n",
    "rev_index_map={};\n",
    "\n",
    "num = 0;\n",
    "for i in range(feature_num):\n",
    "    curr_line = re.split(\": |, \",re.sub(\".\\n|\\.\",\"\",feature_file.readline()));\n",
    "    \n",
    "    index_map[i] = curr_line[0];\n",
    "    \n",
    "    curr_dict = {};\n",
    "    if(curr_line[1]!=\"continuous\"):\n",
    "        curr_dict[\"continuous\"] = False;\n",
    "        for j in range(1,len(curr_line)):\n",
    "            new_index_map[num] = curr_line[0] + \"-\" + curr_line[j];\n",
    "            rev_index_map[curr_line[0] + \"-\" + curr_line[j]] = num;\n",
    "            num += 1;\n",
    "            \n",
    "            curr_dict[curr_line[j]] = 0;\n",
    "    else:\n",
    "        new_index_map[num] = curr_line[0];\n",
    "        rev_index_map[curr_line[0]] = num;\n",
    "        num += 1;\n",
    "        \n",
    "        curr_dict[\"continuous\"] = True;\n",
    "        curr_dict[\"total_val\"] = 0;\n",
    "        curr_dict[\"total_num\"] = 0;\n",
    "    feat_dict[curr_line[0]] = curr_dict;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# parse train data\n",
    "feature_set_x = np.array([]).reshape(0,len(new_index_map));\n",
    "feature_set_y = np.array([]);\n",
    "empty_features = np.array([]).reshape(0,2);\n",
    "for i in range(train_num):\n",
    "    curr_line = re.split(\", |,\",re.sub(\"\\n\",\"\",train_file.readline()));\n",
    "    curr_feat = np.zeros(len(new_index_map));\n",
    "    for j in range (len(curr_line)-1):\n",
    "        if(curr_line[j]==\"?\"):\n",
    "            empty_features = np.vstack((empty_features,[i,index_map[j]]));\n",
    "        else:\n",
    "            if(not feat_dict[index_map[j]][\"continuous\"]):\n",
    "                curr_feat[rev_index_map[index_map[j] + \"-\" + curr_line[j]]] = 1;\n",
    "                feat_dict[index_map[j]][curr_line[j]] += 1;\n",
    "            else:\n",
    "                curr_feat[rev_index_map[index_map[j]]] = int(curr_line[j]);\n",
    "                feat_dict[index_map[j]][\"total_val\"] += int(curr_line[j]);\n",
    "                feat_dict[index_map[j]][\"total_num\"] += 1;\n",
    "    y = 0 if curr_line[12]==\"<=50K\" else 1;\n",
    "    feature_set_x = np.vstack((feature_set_x,curr_feat));\n",
    "    feature_set_y = np.append(feature_set_y,y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# find mean and mode\n",
    "feat_avg_dict = {};\n",
    "for i in range(feature_num):\n",
    "    if(not feat_dict[index_map[i]][\"continuous\"]):\n",
    "        mode = max(feat_dict[index_map[i]],key=feat_dict[index_map[i]].get);\n",
    "        feat_avg_dict[index_map[i]] = mode;\n",
    "    else:\n",
    "        mean = feat_dict[index_map[i]][\"total_val\"]/feat_dict[index_map[i]][\"total_num\"];\n",
    "        feat_avg_dict[index_map[i]] = mean;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# backfill train data\n",
    "for i in range(len(empty_features)):\n",
    "    index = empty_features[i];\n",
    "    if(feat_dict[index[1]][\"continuous\"]):\n",
    "        feature_set_x[int(index[0])][rev_index_map[index[1]]] = feat_avg_dict[index[1]];\n",
    "    else:\n",
    "        mode_index = feat_avg_dict[index[1]];\n",
    "        feature_set_x[int(index[0])][rev_index_map[index[1] + \"-\" + mode_index]] = 1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = [[1,1],[2,2],[3,3],[4,4],[5,5],[6,6],[7,7],[8,8],[9,9],[0,0]];\n",
    "b = [1,2,3,4,5,6,7,8,9,0];\n",
    "\n",
    "ab_state = np.random.get_state();\n",
    "np.random.shuffle(a);\n",
    "np.random.set_state(ab_state);\n",
    "np.random.shuffle(b);\n",
    "\n",
    "print(a);\n",
    "print(b);\n",
    "\n",
    "absplit = math.floor(10*0.3);\n",
    "\n",
    "a_x = a[absplit:]\n",
    "a_y = a[absplit:]\n",
    "\n",
    "b_x = b[:absplit]\n",
    "b_y = b[:absplit]\n",
    "\n",
    "print(a_x);\n",
    "print(a_y);\n",
    "print(b_x);\n",
    "print(b_y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split data\n",
    "r_state = np.random.get_state();\n",
    "np.random.shuffle(feature_set_x);\n",
    "np.random.set_state(r_state);\n",
    "np.random.shuffle(feature_set_y)\n",
    "\n",
    "split = math.floor(train_num*0.3);\n",
    "\n",
    "train_x = feature_set_x[split:];\n",
    "train_y = feature_set_y[split:];\n",
    "\n",
    "validate_x = feature_set_x[:split];\n",
    "validate_y = feature_set_y[:split];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# max_depth\n",
    "d_xaxis = np.array([]);\n",
    "d_train_err = np.array([]);\n",
    "d_validate_err = np.array([]);\n",
    "for i in range(1,30):\n",
    "    d_xaxis = np.append(d_xaxis, i);\n",
    "    \n",
    "    clf = DecisionTreeClassifier(max_depth=i);\n",
    "    clf.fit(train_x,y=train_y);\n",
    "    \n",
    "    y = clf.predict(train_x);\n",
    "    train_err = (len(train_y)-np.sum(np.equal(y,train_y)))/len(train_y);\n",
    "    d_train_err = np.append(d_train_err,train_err);\n",
    "    \n",
    "    y = clf.predict(validate_x);\n",
    "    validate_err = (len(validate_y)-np.sum(np.equal(y,validate_y)))/len(validate_y)\n",
    "    d_validate_err = np.append(d_validate_err,validate_err);\n",
    "\n",
    "green_patch = mpatches.Patch(color='green', label='Validation Error');\n",
    "blue_patch = mpatches.Patch(color='blue', label='Training Error')\n",
    "\n",
    "plt.figure();\n",
    "plt.title(\"Max Tree Depth vs. Error Rate\");\n",
    "plt.xlabel(\"Max Depth\");\n",
    "plt.ylabel(\"Error Rate\");\n",
    "plt.legend(handles=[green_patch, blue_patch])\n",
    "plt.plot(d_xaxis, d_train_err);\n",
    "plt.plot(d_xaxis, d_validate_err);\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# min samples leaf\n",
    "l_xaxis = np.array([]);\n",
    "l_train_err = np.array([]);\n",
    "l_validate_err = np.array([]);\n",
    "for i in range(1,50):\n",
    "    l_xaxis = np.append(l_xaxis, i);\n",
    "    \n",
    "    clf = DecisionTreeClassifier(min_samples_leaf=i);\n",
    "    clf.fit(train_x,y=train_y);\n",
    "    \n",
    "    y = clf.predict(train_x);\n",
    "    train_err = (len(train_y)-np.sum(np.equal(y,train_y)))/len(train_y);\n",
    "    l_train_err = np.append(l_train_err,train_err);\n",
    "    \n",
    "    y = clf.predict(validate_x);\n",
    "    validate_err = (len(validate_y)-np.sum(np.equal(y,validate_y)))/len(validate_y)\n",
    "    l_validate_err = np.append(l_validate_err,validate_err);\n",
    "\n",
    "green_patch = mpatches.Patch(color='green', label='Validation Error');\n",
    "blue_patch = mpatches.Patch(color='blue', label='Training Error')\n",
    "\n",
    "plt.figure();\n",
    "plt.title(\"Min Samples Required to be a Leaf vs. Error Rate\");\n",
    "plt.xlabel(\"Min Samples\");\n",
    "plt.ylabel(\"Error Rate\");\n",
    "plt.legend(handles=[green_patch, blue_patch])\n",
    "plt.plot(l_xaxis, l_train_err);\n",
    "plt.plot(l_xaxis, l_validate_err);\n",
    "plt.show();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
